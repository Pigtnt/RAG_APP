# import os
# import time
# import re
# from langchain_community.document_loaders import PDFPlumberLoader
# from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_huggingface import HuggingFaceEmbeddings
# from langchain_chroma import Chroma
# from langchain_google_genai import ChatGoogleGenerativeAI
# from langchain_core.prompts import PromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# from sentence_transformers import CrossEncoder
# from dotenv import load_dotenv
#
# load_dotenv()
#
# # ==========================================
# # ğŸ› ï¸ è¨­å®šå€ï¼šé›™æ¨¡å‹æ¶æ§‹ (Player vs Judge)
# # ==========================================
# file_path = "fubon.pdf"
#
# # ğŸŸ¢ çƒå“¡ï¼šè² è²¬å›ç­”å•é¡Œ (è¼•é‡ç´š)
# # æ³¨æ„ï¼šè‹¥ API å ±éŒ¯ï¼Œè«‹å˜—è©¦åŠ ä¸Š "models/" å‰ç¶´ï¼Œå¦‚ "models/gemma-3-12b-it"
# GENERATOR_MODEL = "gemma-3-12b-it"
#
# # ğŸŸ¢ è£åˆ¤ï¼šè² è²¬è©•åˆ† (é‡é‡ç´šï¼Œé‚è¼¯æ›´å¼·)
# JUDGE_MODEL = "gemma-3-27b-it"
#
# print(f"ğŸš€ å•Ÿå‹• RAG ç³»çµ± (é›™æ¨¡å‹å”ä½œç‰ˆ)...")
# print(f"ğŸƒ çƒå“¡æ¨¡å‹ (Generator): {GENERATOR_MODEL}")
# print(f"ğŸ‘¨â€âš–ï¸ è£åˆ¤æ¨¡å‹ (Judge):     {JUDGE_MODEL}")
# print(f"ğŸ“„ è™•ç†æª”æ¡ˆ: {file_path}")
#
#
# # ==========================================
# # 1. è³‡æ–™è™•ç†
# # ==========================================
# def load_and_split_pdf(file_path):
#     print("   [ç³»çµ±æç¤º] PDFPlumber è§£æä¸­...")
#     loader = PDFPlumberLoader(file_path)
#     docs = loader.load()
#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)
#     splits = text_splitter.split_documents(docs)
#     return splits
#
#
# # ==========================================
# # 2. è¼‰å…¥æª¢ç´¢æ¨¡å‹
# # ==========================================
# print("   [ç³»çµ±æç¤º] è¼‰å…¥ Embedding & Cross-Encoder...")
# embedding_model = HuggingFaceEmbeddings(
#     model_name="intfloat/multilingual-e5-large",
#     model_kwargs={'device': 'cpu'},
#     encode_kwargs={'normalize_embeddings': True}
# )
# reranker_model = CrossEncoder('BAAI/bge-reranker-base', device='cpu')
#
# # ==========================================
# # 3. å»ºç«‹å‘é‡è³‡æ–™åº«
# # ==========================================
# pdf_splits = load_and_split_pdf(file_path)
# vectorstore = Chroma.from_documents(
#     documents=pdf_splits,
#     embedding=embedding_model,
#     collection_name="fubon_dual_model_demo"  # æ”¹åé¿å…è¡çª
# )
# retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
#
# # ==========================================
# # 4. è¨­å®š LLM (é›™æ¨¡å‹å¯¦ä¾‹åŒ–)
# # ==========================================
#
# # ğŸŸ¢ 1. çƒå“¡ (Generator) - Gemma 3 12B
# llm_generator = ChatGoogleGenerativeAI(
#     model=GENERATOR_MODEL,
#     temperature=0.1,
#     safety_settings={"HARM_CATEGORY_DANGEROUS_CONTENT": "BLOCK_NONE"}
# )
#
# # ğŸŸ¢ 2. è£åˆ¤ (Judge) - Gemma 3 27B
# llm_judge = ChatGoogleGenerativeAI(
#     model=JUDGE_MODEL,
#     temperature=0.0,  # è£åˆ¤å¿…é ˆçµ•å°å®¢è§€
#     safety_settings={"HARM_CATEGORY_DANGEROUS_CONTENT": "BLOCK_NONE"}
# )
#
# # RAG å›ç­” Prompt
# rag_template = """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„éŠ€è¡Œå®¢æœã€‚è«‹åš´æ ¼ä¾æ“šä¸‹æ–¹ã€ç›¸é—œç‰‡æ®µã€‘å›ç­”ã€å®¢æˆ¶å•é¡Œã€‘ã€‚
# è‹¥ç„¡ç›¸é—œè³‡è¨Šï¼Œè«‹å›ç­”ã€Œæ‰‹å†Šä¸­æœªæåŠã€ã€‚
#
# ã€ç›¸é—œç‰‡æ®µã€‘ï¼š
# {context}
#
# ã€å®¢æˆ¶å•é¡Œã€‘ï¼š
# {question}
#
# å›ç­”:"""
# rag_prompt = PromptTemplate.from_template(rag_template)
#
#
# # ==========================================
# # ğŸŸ¢ RAGAS è©•åˆ†é‚è¼¯ (ä½¿ç”¨ llm_judge / 27B)
# # ==========================================
# def calculate_ragas_score(question, answer, contexts):
#     # Faithfulness (ç„¡å¹»è¦º)
#     faithfulness_prompt = PromptTemplate.from_template("""
#     ä½ æ˜¯ä¸€ä½åš´æ ¼çš„ RAG è©•æ¸¬å“¡ã€‚
#     è«‹æª¢æŸ¥ã€ŒAI å›ç­”ã€æ˜¯å¦åŒ…å«ã€Œåƒè€ƒç‰‡æ®µã€ä¸­æ²’æœ‰çš„å¹»è¦ºè³‡è¨Šã€‚
#
#     ã€åƒè€ƒç‰‡æ®µã€‘ï¼š
#     {contexts}
#
#     ã€AI å›ç­”ã€‘ï¼š
#     {answer}
#
#     è«‹å›å‚³ä¸€å€‹ 0.0 åˆ° 1.0 çš„åˆ†æ•¸ (1.0 ä»£è¡¨å®Œå…¨å¿ å¯¦æ–¼åŸæ–‡ï¼Œç„¡å¹»è¦º)ã€‚
#     åªå›å‚³æ•¸å­—ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
#     """)
#
#     # Relevance (åˆ‡é¡Œåº¦)
#     relevance_prompt = PromptTemplate.from_template("""
#     ä½ æ˜¯ä¸€ä½åš´æ ¼çš„ RAG è©•æ¸¬å“¡ã€‚
#     è«‹è©•åˆ†ã€ŒAI å›ç­”ã€æ˜¯å¦ç²¾æº–å›ç­”äº†ã€Œç”¨æˆ¶å•é¡Œã€ï¼Œä¸”æ²’æœ‰ç­”éæ‰€å•ã€‚
#
#     ã€ç”¨æˆ¶å•é¡Œã€‘ï¼š
#     {question}
#
#     ã€AI å›ç­”ã€‘ï¼š
#     {answer}
#
#     è«‹å›å‚³ä¸€å€‹ 0.0 åˆ° 1.0 çš„åˆ†æ•¸ (1.0 ä»£è¡¨éå¸¸åˆ‡é¡Œ)ã€‚
#     åªå›å‚³æ•¸å­—ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
#     """)
#
#     try:
#         # ğŸ”´ é—œéµï¼šé€™è£¡ä½¿ç”¨ llm_judge (27B) ä¾†åŸ·è¡Œè©•åˆ†
#         f_chain = faithfulness_prompt | llm_judge | StrOutputParser()
#         r_chain = relevance_prompt | llm_judge | StrOutputParser()
#
#         f_str = f_chain.invoke({"contexts": contexts, "answer": answer}).strip()
#         r_str = r_chain.invoke({"question": question, "answer": answer}).strip()
#
#         # è§£ææ•¸å­—
#         f_match = re.findall(r"[-+]?\d*\.\d+|\d+", f_str)
#         r_match = re.findall(r"[-+]?\d*\.\d+|\d+", r_str)
#
#         f_score = float(f_match[0]) if f_match else 0.0
#         r_score = float(r_match[0]) if r_match else 0.0
#
#         return min(f_score, 1.0), min(r_score, 1.0)
#
#     except Exception as e:
#         print(f"   [è©•åˆ†ç³»çµ±éŒ¯èª¤] {e}")
#         return 0.0, 0.0
#
#
# # ==========================================
# # ä¸»æµç¨‹
# # ==========================================
# def run_rag_with_evaluation(query):
#     print(f"\nâ“ æ¸¬è©¦å•é¡Œ: {query}")
#     print("-" * 50)
#
#     # 1. Recall & Rerank
#     initial_docs = retriever.invoke(query)
#     pairs = [[query, doc.page_content] for doc in initial_docs]
#     scores = reranker_model.predict(pairs)
#     scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)
#     top_3_docs = [doc for doc, score in scored_docs[:3]]
#
#     # 2. Generation (ä½¿ç”¨ llm_generator / 12B)
#     context_text = "\n\n".join([doc.page_content for doc in top_3_docs])
#     print(f"ğŸ¤– çƒå“¡ (12B) ç”Ÿæˆå›ç­”ä¸­...")
#
#     # ğŸ”´ é—œéµï¼šé€™è£¡ä½¿ç”¨ llm_generator
#     chain = rag_prompt | llm_generator | StrOutputParser()
#     answer = chain.invoke({"context": context_text, "question": query})
#     print(f"ğŸ’¬ å›ç­”: {answer.strip()}")
#
#     # 3. Evaluation (ä½¿ç”¨ llm_judge / 27B)
#     print("-" * 50)
#     print(f"âš–ï¸ è£åˆ¤ (27B) è©•åˆ†ä¸­...")
#     f_score, r_score = calculate_ragas_score(query, answer, context_text)
#
#     print(f"ğŸ“Š è©•åˆ†å ±å‘Š:")
#     print(f"   â¤ Faithfulness (ç„¡å¹»è¦º): {f_score:.2f} / 1.0")
#     print(f"   â¤ Relevance (åˆ‡é¡Œåº¦):   {r_score:.2f} / 1.0")
#
#     if f_score < 0.8:
#         print("   âš ï¸  è­¦ç¤ºï¼šçƒå“¡å¯èƒ½ç”¢ç”Ÿå¹»è¦ºï¼")
#     elif r_score < 0.5:
#         print("   âš ï¸  è­¦ç¤ºï¼šçƒå“¡ç­”éæ‰€å•ï¼")
#     else:
#         print("   âœ… Passï¼šè¡¨ç¾å„ªè‰¯ã€‚")
#     print("=" * 60)
#
#
# # ==========================================
# # ğŸ“ 10 é¡Œå®Œæ•´æ¸¬è©¦é¡Œåº« (ç°¡å–®ã€é™·é˜±ã€å¦å®š)
# # ==========================================
# questions = [
#     # --- ç°¡å–®é¡Œ ---
#     "è«‹å•å¯Œé‚¦å°Šå¾¡ä¸–ç•Œå¡çš„æ­£å¡å¹´è²»æ˜¯å¤šå°‘éŒ¢ï¼Ÿé™„å¡è¦å¹´è²»å—ï¼Ÿ",
#     "æˆ‘è¦é ç´„é€£å‡æœŸé–“çš„ã€åœ‹å…§æ©Ÿå ´æ¥é€ã€ï¼Œæœ€æ™šéœ€è¦åœ¨å¹¾å€‹å·¥ä½œå¤©å‰é ç´„ï¼Ÿ",
#     "è«‹å•é“è·¯æ•‘æ´æœå‹™å°ˆç·šçš„é›»è©±è™Ÿç¢¼æ˜¯å¤šå°‘ï¼Ÿ",
#
#     # --- é™·é˜±é¡Œ (è€ƒé©— 12B çš„é‚è¼¯) ---
#     "æˆ‘ä¸Šé€±å‰›è²·äº†æ©Ÿç¥¨ï¼Œé‡‘é¡æ˜¯ 12,000 å…ƒï¼Œæˆ‘æ˜¯å¯Œé‚¦ä¸–ç•Œå¡çš„å¡å‹ï¼ˆéç†è²¡æœƒå“¡ï¼‰ï¼Œè«‹å•æˆ‘å¯ä»¥é ç´„å…è²»æ©Ÿå ´æ¥é€å—ï¼Ÿ",
#     "æˆ‘ç”¨å°Šå¾¡ä¸–ç•Œå¡åˆ·äº†æ©Ÿç¥¨ï¼Œä½†æ˜¯æ˜¯åœ¨ 7 å€‹æœˆå‰ï¼ˆç´„ 210 å¤©å‰ï¼‰åˆ·çš„ï¼Œç¾åœ¨è¦å‡ºåœ‹å¯ä»¥ç”¨æ©Ÿå ´å¤–åœåœè»Šå—ï¼Ÿ",
#     "æˆ‘æ˜¯å¯Œé‚¦ç„¡é™å¡æŒå¡äººï¼Œæˆ‘å…’å­ä»Šå¹´ 26 æ­²æœªå©šï¼Œè·Ÿæˆ‘ä¸€èµ·å‡ºåœ‹ï¼Œæˆ‘å¹«ä»–åˆ·äº†å…¨é¡æ©Ÿç¥¨ï¼Œè«‹å•ä»–æœ‰æ—…éŠå¹³å®‰éšªçš„ä¿éšœå—ï¼Ÿ",
#     "æˆ‘æŒæœ‰å¯Œé‚¦ä¸–ç•Œå¡ï¼Œä¸ŠæœŸå¸³å–®ä¸€èˆ¬æ¶ˆè²» 18,000 å…ƒï¼Œè«‹å•æˆ‘å»ã€å°ç£è¯é€šã€åœè»Šå¯ä»¥å…è²»åœå¹¾å°æ™‚ï¼Ÿ",
#
#     # --- å¦å®šé¡Œ (æ’é™¤æ¢æ¬¾) ---
#     "æˆ‘ç‚ºäº†æ¹Šå…å¹´è²»çš„é–€æª»ï¼Œå»å…¨è¯ç¦åˆ©ä¸­å¿ƒè²·äº†å¾ˆå¤šæ±è¥¿ï¼Œè«‹å•é€™äº›æ¶ˆè²»ç®—åœ¨ã€ä¸€èˆ¬æ¶ˆè²»ã€è£¡é¢å—ï¼Ÿ",
#     "æˆ‘å‰›è²·çš„æ–°æ‰‹æ©Ÿè¢«å·äº†ï¼Œå¯ä»¥ç”¨ä¿¡ç”¨å¡çš„ã€å…¨çƒè³¼ç‰©ä¿éšœã€ç”³è«‹ç†è³ å—ï¼Ÿ",
#     "æˆ‘çš„è»Šæœ‰æ”¹è£éï¼Œåº•ç›¤æ¯”è¼ƒä½ï¼ˆé›¢åœ° 15 å…¬åˆ†ï¼‰ï¼Œè»Šå­æ‹‹éŒ¨äº†å¯ä»¥ä½¿ç”¨å…è²»é“è·¯æ•‘æ´æ‹–åŠå—ï¼Ÿ"
# ]
#
# # åŸ·è¡Œæ¸¬è©¦
# for q in questions:
#     run_rag_with_evaluation(q)
#     time.sleep(3)
#
# # æ¸…ç†
# vectorstore.delete_collection()
# print("\nâœ… æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")|
# import os
# import sys  # ç”¨æ–¼ä¸­æ–·ç¨‹å¼
# import time
# import re
# from langchain_community.document_loaders import PDFPlumberLoader
# from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_huggingface import HuggingFaceEmbeddings
# from langchain_chroma import Chroma
# from langchain_google_genai import ChatGoogleGenerativeAI
# from langchain_core.prompts import PromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# from sentence_transformers import CrossEncoder
# from dotenv import load_dotenv
#
# # ğŸŸ¢ æ–°å¢ï¼šæ··åˆæª¢ç´¢éœ€è¦çš„æ¨¡çµ„
# # æ··åˆæª¢ç´¢éœ€è¦é€™å…©å€‹
# from langchain.retrievers import EnsembleRetriever
# from langchain_community.retrievers import BM25Retriever
#
# load_dotenv()
#
# # ğŸ” RAG å·¥ç¨‹å¸«çš„é™¤éŒ¯æª¢æŸ¥ï¼šç¢ºèª Key æ˜¯å¦å­˜åœ¨
# GOOGLE_API_KEY = os.getenv("GEMINI_API_KEY")
#
# if not GOOGLE_API_KEY:
#     print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° GOOGLE_API_KEYï¼")
#     print("   è«‹ç¢ºèªç›®éŒ„ä¸‹æ˜¯å¦æœ‰ .env æª”æ¡ˆï¼Œä¸”å…§å®¹åŒ…å« GOOGLE_API_KEY=AIza...")
#     sys.exit(1) # ç›´æ¥åœæ­¢ï¼Œé¿å…å¾Œé¢å ±éŒ¯
#
#
# # ==========================================
# # ğŸ› ï¸ è¨­å®šå€ï¼šé›™æ¨¡å‹æ¶æ§‹ (Player vs Judge)
# # ==========================================
# file_path = "fubon.pdf"
#
# # ğŸŸ¢ çƒå“¡ï¼šè² è²¬å›ç­”å•é¡Œ (è¼•é‡ç´š)
# GENERATOR_MODEL = "gemma-3-12b-it"
#
# # ğŸŸ¢ è£åˆ¤ï¼šè² è²¬è©•åˆ† (é‡é‡ç´šï¼Œé‚è¼¯æ›´å¼·)
# JUDGE_MODEL = "gemma-3-27b-it"
#
# print(f"ğŸš€ å•Ÿå‹• RAG ç³»çµ± (Level 5 - Hybrid Search æ··åˆæª¢ç´¢ç‰ˆ)...")
# print(f"ğŸƒ çƒå“¡æ¨¡å‹ (Generator): {GENERATOR_MODEL}")
# print(f"ğŸ‘¨â€âš–ï¸ è£åˆ¤æ¨¡å‹ (Judge):     {JUDGE_MODEL}")
# print(f"ğŸ“„ è™•ç†æª”æ¡ˆ: {file_path}")
#
#
# # ==========================================
# # 1. è³‡æ–™è™•ç†
# # ==========================================
# def load_and_split_pdf(file_path):
#     print("   [ç³»çµ±æç¤º] PDFPlumber è§£æä¸­...")
#     loader = PDFPlumberLoader(file_path)
#     docs = loader.load()
#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)
#     splits = text_splitter.split_documents(docs)
#     return splits
#
#
# # ==========================================
# # 2. è¼‰å…¥æª¢ç´¢æ¨¡å‹
# # ==========================================
# print("   [ç³»çµ±æç¤º] è¼‰å…¥ Embedding & Cross-Encoder...")
# embedding_model = HuggingFaceEmbeddings(
#     model_name="intfloat/multilingual-e5-large",
#     model_kwargs={'device': 'cpu'},
#     encode_kwargs={'normalize_embeddings': True}
# )
# reranker_model = CrossEncoder('BAAI/bge-reranker-base', device='cpu')
#
# # ==========================================
# # 3. å»ºç«‹ Hybrid Retriever (é—œéµä¿®æ”¹å€) ğŸŸ¢
# # ==========================================
# pdf_splits = load_and_split_pdf(file_path)
#
# # --- A. å»ºç«‹å‘é‡æª¢ç´¢ (Vector Search) ---
# # æ“…é•·ï¼šèªæ„ç†è§£ (ä¾‹å¦‚çŸ¥é“ã€Œè²»ç”¨ã€è·Ÿã€ŒéŒ¢ã€æœ‰é—œ)
# print("   [ç³»çµ±æç¤º] å»ºç«‹ Chroma å‘é‡ç´¢å¼•...")
# vectorstore = Chroma.from_documents(
#     documents=pdf_splits,
#     embedding=embedding_model,
#     collection_name="fubon_hybrid_final"  # æ”¹åé¿å…è¡çª
# )
# chroma_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
#
# # --- B. å»ºç«‹é—œéµå­—æª¢ç´¢ (Keyword Search - BM25) ---
# # æ“…é•·ï¼šç²¾ç¢ºåŒ¹é… (ä¾‹å¦‚ç²¾æº–æŠ“åˆ° "180å¤©", "å°Šå¾¡ä¸–ç•Œå¡", "20,000" é€™äº›å­—)
# print("   [ç³»çµ±æç¤º] å»ºç«‹ BM25 é—œéµå­—ç´¢å¼•...")
# bm25_retriever = BM25Retriever.from_documents(pdf_splits)
# bm25_retriever.k = 10  # è®“å®ƒä¹ŸæŠ“ 10 ç­†
#
# # --- C. èåˆ (Ensemble) ---
# # å°‡å…©è€…çš„çµæœçµåˆï¼Œæ¬Šé‡å„åŠ (0.5/0.5)
# print("   [ç³»çµ±æç¤º] å•Ÿå‹• Hybrid Ensemble (æ··åˆæª¢ç´¢)...")
# ensemble_retriever = EnsembleRetriever(
#     retrievers=[chroma_retriever, bm25_retriever],
#     weights=[0.5, 0.5]
# )
#
# # å°‡æœ€çµ‚çš„æª¢ç´¢å™¨è¨­å®šç‚ºæ··åˆæª¢ç´¢å™¨
# retriever = ensemble_retriever
#
# # ==========================================
# # 4. è¨­å®š LLM (é›™æ¨¡å‹å¯¦ä¾‹åŒ–)
# # ==========================================
# print("   [ç³»çµ±æç¤º] åˆå§‹åŒ– LLM æ¨¡å‹ä¸­...")
#
# try:
#     # âœ… ä¿®æ”¹é»ï¼šé¡¯å¼å‚³å…¥ google_api_keyï¼Œé¿å…è§¸ç™¼ DefaultCredentialsError
#     llm_generator = ChatGoogleGenerativeAI(
#         model=GENERATOR_MODEL,
#         temperature=0.1,
#         google_api_key=GOOGLE_API_KEY  # å¼·åˆ¶æŒ‡å®š Key
#     )
#
#     llm_judge = ChatGoogleGenerativeAI(
#         model=JUDGE_MODEL,
#         temperature=0.0,
#         google_api_key=GOOGLE_API_KEY  # å¼·åˆ¶æŒ‡å®š Key
#     )
# except Exception as e:
#     print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
#     sys.exit(1)
#
#
# # RAG å›ç­” Prompt
# rag_template = """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„éŠ€è¡Œå®¢æœã€‚è«‹åš´æ ¼ä¾æ“šä¸‹æ–¹ã€ç›¸é—œç‰‡æ®µã€‘å›ç­”ã€å®¢æˆ¶å•é¡Œã€‘ã€‚
# è‹¥ç„¡ç›¸é—œè³‡è¨Šï¼Œè«‹å›ç­”ã€Œæ‰‹å†Šä¸­æœªæåŠã€ã€‚
#
# ã€ç›¸é—œç‰‡æ®µã€‘ï¼š
# {context}
#
# ã€å®¢æˆ¶å•é¡Œã€‘ï¼š
# {question}
#
# å›ç­”:"""
# rag_prompt = PromptTemplate.from_template(rag_template)
#
#
# # ==========================================
# # ğŸŸ¢ RAGAS è©•åˆ†é‚è¼¯ (ä½¿ç”¨ llm_judge / 27B)
# # ==========================================
# def calculate_ragas_score(question, answer, contexts):
#     # Faithfulness (ç„¡å¹»è¦º)
#     faithfulness_prompt = PromptTemplate.from_template("""
#     ä½ æ˜¯ä¸€ä½åš´æ ¼çš„ RAG è©•æ¸¬å“¡ã€‚
#     è«‹æª¢æŸ¥ã€ŒAI å›ç­”ã€æ˜¯å¦åŒ…å«ã€Œåƒè€ƒç‰‡æ®µã€ä¸­æ²’æœ‰çš„å¹»è¦ºè³‡è¨Šã€‚
#
#     ã€åƒè€ƒç‰‡æ®µã€‘ï¼š
#     {contexts}
#
#     ã€AI å›ç­”ã€‘ï¼š
#     {answer}
#
#     è«‹å›å‚³ä¸€å€‹ 0.0 åˆ° 1.0 çš„åˆ†æ•¸ (1.0 ä»£è¡¨å®Œå…¨å¿ å¯¦æ–¼åŸæ–‡ï¼Œç„¡å¹»è¦º)ã€‚
#     åªå›å‚³æ•¸å­—ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
#     """)
#
#     # Relevance (åˆ‡é¡Œåº¦)
#     relevance_prompt = PromptTemplate.from_template("""
#     ä½ æ˜¯ä¸€ä½åš´æ ¼çš„ RAG è©•æ¸¬å“¡ã€‚
#     è«‹è©•åˆ†ã€ŒAI å›ç­”ã€æ˜¯å¦ç²¾æº–å›ç­”äº†ã€Œç”¨æˆ¶å•é¡Œã€ï¼Œä¸”æ²’æœ‰ç­”éæ‰€å•ã€‚
#
#     ã€ç”¨æˆ¶å•é¡Œã€‘ï¼š
#     {question}
#
#     ã€AI å›ç­”ã€‘ï¼š
#     {answer}
#
#     è«‹å›å‚³ä¸€å€‹ 0.0 åˆ° 1.0 çš„åˆ†æ•¸ (1.0 ä»£è¡¨éå¸¸åˆ‡é¡Œ)ã€‚
#     åªå›å‚³æ•¸å­—ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
#     """)
#
#     try:
#         # ğŸ”´ é—œéµï¼šé€™è£¡ä½¿ç”¨ llm_judge (27B) ä¾†åŸ·è¡Œè©•åˆ†
#         f_chain = faithfulness_prompt | llm_judge | StrOutputParser()
#         r_chain = relevance_prompt | llm_judge | StrOutputParser()
#
#         f_str = f_chain.invoke({"contexts": contexts, "answer": answer}).strip()
#         r_str = r_chain.invoke({"question": question, "answer": answer}).strip()
#
#         # è§£ææ•¸å­—
#         f_match = re.findall(r"[-+]?\d*\.\d+|\d+", f_str)
#         r_match = re.findall(r"[-+]?\d*\.\d+|\d+", r_str)
#
#         f_score = float(f_match[0]) if f_match else 0.0
#         r_score = float(r_match[0]) if r_match else 0.0
#
#         return min(f_score, 1.0), min(r_score, 1.0)
#
#     except Exception as e:
#         print(f"   [è©•åˆ†ç³»çµ±éŒ¯èª¤] {e}")
#         return 0.0, 0.0
#
#
# # ==========================================
# # ä¸»æµç¨‹
# # ==========================================
# def run_rag_with_evaluation(query):
#     print(f"\nâ“ æ¸¬è©¦å•é¡Œ: {query}")
#     print("-" * 50)
#
#     # 1. Recall (Hybrid Search)
#     # é€™è£¡çš„ invoke æœƒåŒæ™‚è·‘ Vector å’Œ BM25ï¼Œç„¶å¾Œæ··åˆçµæœ
#     initial_docs = retriever.invoke(query)
#
#     # 2. Re-rank (Cross-Encoder æ±ºé¸)
#     # å°‡ Hybrid æŠ“å›ä¾†çš„ 20 ç­† (10+10) é€²è¡Œç²¾æº–æ’åº
#     pairs = [[query, doc.page_content] for doc in initial_docs]
#     scores = reranker_model.predict(pairs)
#     scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)
#     top_3_docs = [doc for doc, score in scored_docs[:3]]
#
#     # 3. Generation (ä½¿ç”¨ llm_generator / 12B)
#     context_text = "\n\n".join([doc.page_content for doc in top_3_docs])
#     print(f"ğŸ¤– çƒå“¡ (12B) ç”Ÿæˆå›ç­”ä¸­...")
#
#     chain = rag_prompt | llm_generator | StrOutputParser()
#     answer = chain.invoke({"context": context_text, "question": query})
#     print(f"ğŸ’¬ å›ç­”: {answer.strip()}")
#
#     # 4. Evaluation (ä½¿ç”¨ llm_judge / 27B)
#     print("-" * 50)
#     print(f"âš–ï¸ è£åˆ¤ (27B) è©•åˆ†ä¸­...")
#     f_score, r_score = calculate_ragas_score(query, answer, context_text)
#
#     print(f"ğŸ“Š è©•åˆ†å ±å‘Š:")
#     print(f"   â¤ Faithfulness (ç„¡å¹»è¦º): {f_score:.2f} / 1.0")
#     print(f"   â¤ Relevance (åˆ‡é¡Œåº¦):   {r_score:.2f} / 1.0")
#
#     if f_score < 0.8:
#         print("   âš ï¸  è­¦ç¤ºï¼šçƒå“¡å¯èƒ½ç”¢ç”Ÿå¹»è¦ºï¼")
#     elif r_score < 0.5:
#         print("   âš ï¸  è­¦ç¤ºï¼šçƒå“¡ç­”éæ‰€å•ï¼")
#     else:
#         print("   âœ… Passï¼šè¡¨ç¾å„ªè‰¯ã€‚")
#     print("=" * 60)
#
#
# # ==========================================
# # ğŸ“ 10 é¡Œå®Œæ•´æ¸¬è©¦é¡Œåº« (ç°¡å–®ã€é™·é˜±ã€å¦å®š)
# # ==========================================
# questions = [
#     # --- ç°¡å–®é¡Œ ---
#     "è«‹å•å¯Œé‚¦å°Šå¾¡ä¸–ç•Œå¡çš„æ­£å¡å¹´è²»æ˜¯å¤šå°‘éŒ¢ï¼Ÿé™„å¡è¦å¹´è²»å—ï¼Ÿ",
#     "æˆ‘è¦é ç´„é€£å‡æœŸé–“çš„ã€åœ‹å…§æ©Ÿå ´æ¥é€ã€ï¼Œæœ€æ™šéœ€è¦åœ¨å¹¾å€‹å·¥ä½œå¤©å‰é ç´„ï¼Ÿ",
#     "è«‹å•é“è·¯æ•‘æ´æœå‹™å°ˆç·šçš„é›»è©±è™Ÿç¢¼æ˜¯å¤šå°‘ï¼Ÿ",
#
#     # --- é™·é˜±é¡Œ (è€ƒé©— 12B çš„é‚è¼¯) ---
#     "æˆ‘ä¸Šé€±å‰›è²·äº†æ©Ÿç¥¨ï¼Œé‡‘é¡æ˜¯ 12,000 å…ƒï¼Œæˆ‘æ˜¯å¯Œé‚¦ä¸–ç•Œå¡çš„å¡å‹ï¼ˆéç†è²¡æœƒå“¡ï¼‰ï¼Œè«‹å•æˆ‘å¯ä»¥é ç´„å…è²»æ©Ÿå ´æ¥é€å—ï¼Ÿ",
#     "æˆ‘ç”¨å°Šå¾¡ä¸–ç•Œå¡åˆ·äº†æ©Ÿç¥¨ï¼Œä½†æ˜¯æ˜¯åœ¨ 7 å€‹æœˆå‰ï¼ˆç´„ 210 å¤©å‰ï¼‰åˆ·çš„ï¼Œç¾åœ¨è¦å‡ºåœ‹å¯ä»¥ç”¨æ©Ÿå ´å¤–åœåœè»Šå—ï¼Ÿ",
#     "æˆ‘æ˜¯å¯Œé‚¦ç„¡é™å¡æŒå¡äººï¼Œæˆ‘å…’å­ä»Šå¹´ 26 æ­²æœªå©šï¼Œè·Ÿæˆ‘ä¸€èµ·å‡ºåœ‹ï¼Œæˆ‘å¹«ä»–åˆ·äº†å…¨é¡æ©Ÿç¥¨ï¼Œè«‹å•ä»–æœ‰æ—…éŠå¹³å®‰éšªçš„ä¿éšœå—ï¼Ÿ",
#     "æˆ‘æŒæœ‰å¯Œé‚¦ä¸–ç•Œå¡ï¼Œä¸ŠæœŸå¸³å–®ä¸€èˆ¬æ¶ˆè²» 18,000 å…ƒï¼Œè«‹å•æˆ‘å»ã€å°ç£è¯é€šã€åœè»Šå¯ä»¥å…è²»åœå¹¾å°æ™‚ï¼Ÿ",
#
#     # --- å¦å®šé¡Œ (æ’é™¤æ¢æ¬¾) ---
#     "æˆ‘ç‚ºäº†æ¹Šå…å¹´è²»çš„é–€æª»ï¼Œå»å…¨è¯ç¦åˆ©ä¸­å¿ƒè²·äº†å¾ˆå¤šæ±è¥¿ï¼Œè«‹å•é€™äº›æ¶ˆè²»ç®—åœ¨ã€ä¸€èˆ¬æ¶ˆè²»ã€è£¡é¢å—ï¼Ÿ",
#     "æˆ‘å‰›è²·çš„æ–°æ‰‹æ©Ÿè¢«å·äº†ï¼Œå¯ä»¥ç”¨ä¿¡ç”¨å¡çš„ã€å…¨çƒè³¼ç‰©ä¿éšœã€ç”³è«‹ç†è³ å—ï¼Ÿ",
#     "æˆ‘çš„è»Šæœ‰æ”¹è£éï¼Œåº•ç›¤æ¯”è¼ƒä½ï¼ˆé›¢åœ° 15 å…¬åˆ†ï¼‰ï¼Œè»Šå­æ‹‹éŒ¨äº†å¯ä»¥ä½¿ç”¨å…è²»é“è·¯æ•‘æ´æ‹–åŠå—ï¼Ÿ"
# ]
#
# # åŸ·è¡Œæ¸¬è©¦
# for q in questions:
#     run_rag_with_evaluation(q)
#     time.sleep(3)
#
# # æ¸…ç†
# vectorstore.delete_collection()
# print("\nâœ… æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")
import os
import sys  # ç”¨æ–¼ä¸­æ–·ç¨‹å¼
import time
import re
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from sentence_transformers import CrossEncoder
from dotenv import load_dotenv

# ğŸŸ¢ æ–°å¢ï¼šæ··åˆæª¢ç´¢éœ€è¦çš„æ¨¡çµ„
# æ··åˆæª¢ç´¢éœ€è¦é€™å…©å€‹
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

load_dotenv()

# ğŸ” RAG å·¥ç¨‹å¸«çš„é™¤éŒ¯æª¢æŸ¥ï¼šç¢ºèª Key æ˜¯å¦å­˜åœ¨
GOOGLE_API_KEY = os.getenv("GEMINI_API_KEY")

if not GOOGLE_API_KEY:
    print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° GOOGLE_API_KEYï¼")
    print("   è«‹ç¢ºèªç›®éŒ„ä¸‹æ˜¯å¦æœ‰ .env æª”æ¡ˆï¼Œä¸”å…§å®¹åŒ…å« GOOGLE_API_KEY=AIza...")
    sys.exit(1) # ç›´æ¥åœæ­¢ï¼Œé¿å…å¾Œé¢å ±éŒ¯


# ==========================================
# ğŸ› ï¸ è¨­å®šå€ï¼šé›™æ¨¡å‹æ¶æ§‹ (Player vs Judge)
# ==========================================
file_path = "fubon.pdf"

# ğŸŸ¢ çƒå“¡ï¼šè² è²¬å›ç­”å•é¡Œ (è¼•é‡ç´š)
GENERATOR_MODEL = "gemma-3-12b-it"

# ğŸŸ¢ è£åˆ¤ï¼šè² è²¬è©•åˆ† (é‡é‡ç´šï¼Œé‚è¼¯æ›´å¼·)
JUDGE_MODEL = "gemma-3-27b-it"

print(f"ğŸš€ å•Ÿå‹• RAG ç³»çµ± (Level 5 - Hybrid Search æ··åˆæª¢ç´¢ç‰ˆ)...")
print(f"ğŸƒ çƒå“¡æ¨¡å‹ (Generator): {GENERATOR_MODEL}")
print(f"ğŸ‘¨â€âš–ï¸ è£åˆ¤æ¨¡å‹ (Judge):     {JUDGE_MODEL}")
print(f"ğŸ“„ è™•ç†æª”æ¡ˆ: {file_path}")


# ==========================================
# 1. è³‡æ–™è™•ç†
# ==========================================
def load_and_split_pdf(file_path):
    print("   [ç³»çµ±æç¤º] PDFPlumber è§£æä¸­...")
    loader = PDFPlumberLoader(file_path)
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)
    splits = text_splitter.split_documents(docs)
    return splits


# ==========================================
# 2. è¼‰å…¥æª¢ç´¢æ¨¡å‹
# ==========================================
print("   [ç³»çµ±æç¤º] è¼‰å…¥ Embedding & Cross-Encoder...")
embedding_model = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-large",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)
reranker_model = CrossEncoder('BAAI/bge-reranker-base', device='cpu')

# ==========================================
# 3. å»ºç«‹ Hybrid Retriever (é—œéµä¿®æ”¹å€) ğŸŸ¢
# ==========================================
pdf_splits = load_and_split_pdf(file_path)

# --- A. å»ºç«‹å‘é‡æª¢ç´¢ (Vector Search) ---
# æ“…é•·ï¼šèªæ„ç†è§£ (ä¾‹å¦‚çŸ¥é“ã€Œè²»ç”¨ã€è·Ÿã€ŒéŒ¢ã€æœ‰é—œ)
print("   [ç³»çµ±æç¤º] å»ºç«‹ Chroma å‘é‡ç´¢å¼•...")
vectorstore = Chroma.from_documents(
    documents=pdf_splits,
    embedding=embedding_model,
    collection_name="fubon_hybrid_final"  # æ”¹åé¿å…è¡çª
)
chroma_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

# --- B. å»ºç«‹é—œéµå­—æª¢ç´¢ (Keyword Search - BM25) ---
# æ“…é•·ï¼šç²¾ç¢ºåŒ¹é… (ä¾‹å¦‚ç²¾æº–æŠ“åˆ° "180å¤©", "å°Šå¾¡ä¸–ç•Œå¡", "20,000" é€™äº›å­—)
print("   [ç³»çµ±æç¤º] å»ºç«‹ BM25 é—œéµå­—ç´¢å¼•...")
bm25_retriever = BM25Retriever.from_documents(pdf_splits)
bm25_retriever.k = 10  # è®“å®ƒä¹ŸæŠ“ 10 ç­†

# --- C. èåˆ (Ensemble) ---
# å°‡å…©è€…çš„çµæœçµåˆï¼Œæ¬Šé‡å„åŠ (0.5/0.5)
print("   [ç³»çµ±æç¤º] å•Ÿå‹• Hybrid Ensemble (æ··åˆæª¢ç´¢)...")
ensemble_retriever = EnsembleRetriever(
    retrievers=[chroma_retriever, bm25_retriever],
    weights=[0.5, 0.5]
)

# å°‡æœ€çµ‚çš„æª¢ç´¢å™¨è¨­å®šç‚ºæ··åˆæª¢ç´¢å™¨
retriever = ensemble_retriever

# ==========================================
# 4. è¨­å®š LLM (é›™æ¨¡å‹å¯¦ä¾‹åŒ–)
# ==========================================
print("   [ç³»çµ±æç¤º] åˆå§‹åŒ– LLM æ¨¡å‹ä¸­...")

try:
    # âœ… ä¿®æ”¹é»ï¼šé¡¯å¼å‚³å…¥ google_api_keyï¼Œé¿å…è§¸ç™¼ DefaultCredentialsError
    llm_generator = ChatGoogleGenerativeAI(
        model=GENERATOR_MODEL,
        temperature=0.1,
        google_api_key=GOOGLE_API_KEY  # å¼·åˆ¶æŒ‡å®š Key
    )

    llm_judge = ChatGoogleGenerativeAI(
        model=JUDGE_MODEL,
        temperature=0.0,
        google_api_key=GOOGLE_API_KEY  # å¼·åˆ¶æŒ‡å®š Key
    )
except Exception as e:
    print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
    sys.exit(1)


# RAG å›ç­” Prompt
# ==========================================
# ğŸ’ å·¥ç¨‹ç´š Prompt Template (V4.2 - Fix Data Leakage)
# ==========================================
rag_template = """# Role
ä½ æ˜¯ä¸€ä½å°ˆæ¥­ã€é‚è¼¯åš´è¬¹çš„ã€Œå°åŒ—å¯Œé‚¦éŠ€è¡Œé ‚ç´šå¡æ¬Šç›Šå¯©æ ¸å°ˆå“¡ã€ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“š <context> æº–ç¢ºå›ç­”å®¢æˆ¶å•é¡Œã€‚

# Task
è«‹é–±è®€ <context>ï¼Œä¸¦é‡å° <question> é€²è¡Œè³‡æ ¼å¯©æ ¸èˆ‡å›è¦†ã€‚

# Constraints
1. **åš´æ ¼å¼•ç”¨**ï¼šå›ç­”å¿…é ˆåŸºæ–¼ <context> å…§å®¹ï¼Œå›ç­”çµå°¾è«‹æ¨™è¨»ä¾†æºã€‚
2. **æ•¸å€¼æ¯”å°**ï¼šè‹¥å•é¡Œæ¶‰åŠã€Œé‡‘é¡ã€ã€ã€Œå¤©æ•¸ã€ã€ã€Œå¹´é½¡ã€ï¼Œå¿…é ˆåœ¨æ€è€ƒéç¨‹ä¸­èˆ‡æ‰‹å†Šæ¢æ¬¾é€²è¡Œæ¯”å°ã€‚
3. **æ’é™¤æ¢æ¬¾**ï¼šè‹¥å•é¡Œæ¶‰åŠã€Œä¸€èˆ¬æ¶ˆè²»ã€ï¼Œè«‹ç‰¹åˆ¥æª¢æŸ¥ã€Œæ’é™¤é …ç›®ã€ï¼ˆå¦‚ï¼šå…¨è¯ã€ç¨…æ¬¾ã€å­¸è²»ï¼‰ã€‚
4. **èª å¯¦å›ç­”**ï¼šè‹¥ <context> æœªæåŠï¼Œå›ç­”ã€Œæ‰‹å†Šä¸­æœªæåŠã€ã€‚
5. **èªè¨€**ï¼šç¹é«”ä¸­æ–‡ï¼ˆå°ç£ï¼‰ã€‚

# Instruction (CoT)
åœ¨å›ç­”å‰ï¼Œè«‹å‹™å¿…å…ˆé€²è¡Œ <thinking> æ­¥é©Ÿï¼š
1. **è­˜åˆ¥è®Šæ•¸**ï¼šç”¨æˆ¶çš„å¡åˆ¥ã€èº«åˆ†ã€æ¶ˆè²»é‡‘é¡ã€æ™‚é–“é»ã€‚
2. **æŸ¥æ‰¾æ¢æ¬¾**ï¼šåœ¨ <context> ä¸­æ‰¾åˆ°å°æ‡‰è¦å‰‡ã€‚
3. **é‚è¼¯åˆ¤å®š**ï¼š
   - è³‡æ ¼æª¢æŸ¥ï¼šç”¨æˆ¶é‡‘é¡ vs é–€æª»ï¼Ÿ(ä¾‹å¦‚ï¼š8000 < 10000 -> ä¸åˆæ ¼)
   - æœŸé™æª¢æŸ¥ï¼šå¤©æ•¸ vs é™åˆ¶ï¼Ÿ(ä¾‹å¦‚ï¼š100å¤© < 90å¤© -> éæœŸ)
   - æ’é™¤æª¢æŸ¥ï¼šæ˜¯å¦åœ¨æ’é™¤åå–®ï¼Ÿ
4. **ç”Ÿæˆå›ç­”**ï¼šæ ¹æ“šåˆ¤å®šçµæœå›è¦†ã€‚

# Few-Shot Examples (å·²ä¿®æ­£ï¼šä½¿ç”¨éæ¸¬è©¦é›†çš„æ¡ˆä¾‹)
<example>
User: æˆ‘ä¸Šå€‹æœˆåˆ·äº† 5 è¬å…ƒï¼Œå¯ä»¥å…è²»ä½¿ç”¨æ©Ÿå ´è²´è³“å®¤å—ï¼Ÿ
Context: ...ä½¿ç”¨æ©Ÿå ´è²´è³“å®¤éœ€æ”¯ä»˜è²»ç”¨ï¼ŒæƒŸè‹¥ä½¿ç”¨å‰ 90 å¤©å…§åˆ·å¡è³¼è²·æ©Ÿç¥¨æˆ–åœ˜è²»å–®ç­†é” NT$20,000 ä»¥ä¸Šï¼Œå¯äº«å…è²»ç”¨ä¹™æ¬¡...
Answer:
<thinking>
1. ç”¨æˆ¶è®Šæ•¸ï¼šæ¶ˆè²» 50,000 å…ƒï¼ˆä¸€èˆ¬æ¶ˆè²»ï¼Œéæ©Ÿç¥¨ï¼‰ã€‚
2. æ¢æ¬¾é™åˆ¶ï¼šéœ€åˆ·ã€Œæ©Ÿç¥¨æˆ–åœ˜è²»ã€ä¸”å–®ç­†é” NT$20,000ã€‚
3. åˆ¤å®šï¼šé›–ç„¶ 50,000 > 20,000ï¼Œä½†ç”¨æˆ¶æœªèªªæ˜æ˜¯ç”¨æ–¼ã€Œæ©Ÿç¥¨/åœ˜è²»ã€ï¼Œä¸”æ¢æ¬¾æŒ‡å®šé¡åˆ¥ã€‚è‹¥å‡è¨­ç‚ºä¸€èˆ¬æ¶ˆè²»å‰‡ä¸ç¬¦ã€‚
</thinking>
æ ¹æ“šæ‰‹å†Šè¦å®šï¼Œå…è²»ä½¿ç”¨æ©Ÿå ´è²´è³“å®¤çš„æ¢ä»¶ç‚ºã€Œåˆ·ä»˜æ©Ÿç¥¨æˆ–åœ˜è²»ã€å–®ç­†é” NT$20,000ã€‚è‹¥æ‚¨çš„ 5 è¬å…ƒæ¶ˆè²»ä¸¦éç”¨æ–¼æ©Ÿç¥¨æˆ–åœ˜è²»ï¼Œæç„¡æ³•äº«æœ‰å…è²»è³‡æ ¼ã€‚
</example>

<example>
User: æˆ‘è¦ç¹³å°å­©çš„å­¸è²»ï¼Œé€™ç­†éŒ¢ç®—ä¸€èˆ¬æ¶ˆè²»å—ï¼Ÿå¯ä»¥é›†ç´…åˆ©å—ï¼Ÿ
Context: ...ä¸€èˆ¬æ¶ˆè²»å®šç¾©...ä¸åŒ…å«ï¼šåŸºé‡‘ã€ç¨…æ¬¾ã€å­¸è²»ã€å…¬ç”¨äº‹æ¥­è²»...
Answer:
<thinking>
1. ç”¨æˆ¶è®Šæ•¸ï¼šç¹³å­¸è²»ã€‚
2. æ¢æ¬¾é™åˆ¶ï¼šä¸€èˆ¬æ¶ˆè²»æ’é™¤é …ç›®åŒ…å«ã€Œå­¸è²»ã€ã€‚
3. åˆ¤å®šï¼šå­¸è²»å±¬æ–¼æ’é™¤é …ç›® -> ä¸ç®—ä¸€èˆ¬æ¶ˆè²» -> ä¸çµ¦ç´…åˆ©ã€‚
</thinking>
æ ¹æ“šä¸€èˆ¬æ¶ˆè²»å®šç¾©ï¼Œå­¸è²»å±¬æ–¼ã€Œæ’é™¤é …ç›®ã€ï¼Œå› æ­¤ä¸ç®—åœ¨ä¸€èˆ¬æ¶ˆè²»å…§ï¼Œä¹Ÿç„¡æ³•è¨ˆç®—ç´…åˆ©é»æ•¸ã€‚
</example>

# Context
{context}

# Question
{question}

Answer:"""

rag_prompt = PromptTemplate.from_template(rag_template)


# ==========================================
# ğŸŸ¢ RAGAS è©•åˆ†é‚è¼¯ (ä½¿ç”¨ llm_judge / 27B)
# ==========================================
def calculate_ragas_score(question, answer, contexts):
    # Faithfulness (ç„¡å¹»è¦º)
    faithfulness_prompt = PromptTemplate.from_template("""
    ä½ æ˜¯ä¸€ä½åš´æ ¼çš„ RAG è©•æ¸¬å“¡ã€‚
    è«‹æª¢æŸ¥ã€ŒAI å›ç­”ã€æ˜¯å¦åŒ…å«ã€Œåƒè€ƒç‰‡æ®µã€ä¸­æ²’æœ‰çš„å¹»è¦ºè³‡è¨Šã€‚

    ã€åƒè€ƒç‰‡æ®µã€‘ï¼š
    {contexts}

    ã€AI å›ç­”ã€‘ï¼š
    {answer}

    è«‹å›å‚³ä¸€å€‹ 0.0 åˆ° 1.0 çš„åˆ†æ•¸ (1.0 ä»£è¡¨å®Œå…¨å¿ å¯¦æ–¼åŸæ–‡ï¼Œç„¡å¹»è¦º)ã€‚
    åªå›å‚³æ•¸å­—ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
    """)

    # Relevance (åˆ‡é¡Œåº¦)
    relevance_prompt = PromptTemplate.from_template("""
    ä½ æ˜¯ä¸€ä½åš´æ ¼çš„ RAG è©•æ¸¬å“¡ã€‚
    è«‹è©•åˆ†ã€ŒAI å›ç­”ã€æ˜¯å¦ç²¾æº–å›ç­”äº†ã€Œç”¨æˆ¶å•é¡Œã€ï¼Œä¸”æ²’æœ‰ç­”éæ‰€å•ã€‚

    ã€ç”¨æˆ¶å•é¡Œã€‘ï¼š
    {question}

    ã€AI å›ç­”ã€‘ï¼š
    {answer}

    è«‹å›å‚³ä¸€å€‹ 0.0 åˆ° 1.0 çš„åˆ†æ•¸ (1.0 ä»£è¡¨éå¸¸åˆ‡é¡Œ)ã€‚
    åªå›å‚³æ•¸å­—ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
    """)

    try:
        # ğŸ”´ é—œéµï¼šé€™è£¡ä½¿ç”¨ llm_judge (27B) ä¾†åŸ·è¡Œè©•åˆ†
        f_chain = faithfulness_prompt | llm_judge | StrOutputParser()
        r_chain = relevance_prompt | llm_judge | StrOutputParser()

        f_str = f_chain.invoke({"contexts": contexts, "answer": answer}).strip()
        r_str = r_chain.invoke({"question": question, "answer": answer}).strip()

        # è§£ææ•¸å­—
        f_match = re.findall(r"[-+]?\d*\.\d+|\d+", f_str)
        r_match = re.findall(r"[-+]?\d*\.\d+|\d+", r_str)

        f_score = float(f_match[0]) if f_match else 0.0
        r_score = float(r_match[0]) if r_match else 0.0

        return min(f_score, 1.0), min(r_score, 1.0)

    except Exception as e:
        print(f"   [è©•åˆ†ç³»çµ±éŒ¯èª¤] {e}")
        return 0.0, 0.0


# ==========================================
# ä¸»æµç¨‹ (ä¿®æ”¹ç‰ˆï¼šæ”¯æ´ Streaming)
# ==========================================
def run_rag_with_evaluation(query):
    print(f"\nâ“ æ¸¬è©¦å•é¡Œ: {query}")
    print("-" * 50)

    # 1. Recall (Hybrid Search)
    # é€™è£¡çš„ invoke æœƒåŒæ™‚è·‘ Vector å’Œ BM25ï¼Œç„¶å¾Œæ··åˆçµæœ
    initial_docs = retriever.invoke(query)

    # 2. Re-rank (Cross-Encoder æ±ºé¸)
    # å°‡ Hybrid æŠ“å›ä¾†çš„ 20 ç­† (10+10) é€²è¡Œç²¾æº–æ’åº
    pairs = [[query, doc.page_content] for doc in initial_docs]
    scores = reranker_model.predict(pairs)
    scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)
    top_3_docs = [doc for doc, score in scored_docs[:3]]

    # 3. Generation (ä½¿ç”¨ llm_generator / 12B)
    context_text = "\n\n".join([doc.page_content for doc in top_3_docs])
    print(f"ğŸ¤– çƒå“¡ (12B) ç”Ÿæˆå›ç­”ä¸­ (Streaming)...")
    print("-" * 20)  # åˆ†éš”ç·šï¼Œæº–å‚™é–‹å§‹æ‰“å­—

    chain = rag_prompt | llm_generator | StrOutputParser()

    # --- ğŸŸ¢ ä¿®æ”¹é–‹å§‹ï¼šå¾ invoke æ”¹æˆ stream ---
    full_answer = ""  # ç”¨ä¾†æ”¶é›†å®Œæ•´çš„å›ç­”ï¼Œçµ¦è£åˆ¤è©•åˆ†ç”¨

    # ä½¿ç”¨ chain.stream é€²è¡Œä¸²æµ
    for chunk in chain.stream({"context": context_text, "question": query}):
        # end="" è®“å®ƒä¸è¦æ›è¡Œï¼Œflush=True ç¢ºä¿å­—æœƒé¦¬ä¸Šè·³å‡ºä¾†
        print(chunk, end="", flush=True)
        full_answer += chunk

    print()  # æœ€å¾Œå°ä¸€å€‹æ›è¡Œ
    # --- ğŸŸ¢ ä¿®æ”¹çµæŸ ---

    # 4. Evaluation (ä½¿ç”¨ llm_judge / 27B)
    # è£åˆ¤å¿…é ˆç­‰çƒå“¡è¬›å®Œè©±æ‰èƒ½è©•åˆ†ï¼Œæ‰€ä»¥é€™è£¡ä¸èƒ½ Stream
    print("-" * 50)
    print(f"âš–ï¸ è£åˆ¤ (27B) è©•åˆ†ä¸­...")

    # æ³¨æ„ï¼šé€™è£¡æŠŠæ”¶é›†å¥½çš„ full_answer å‚³é€²å»
    f_score, r_score = calculate_ragas_score(query, full_answer, context_text)

    print(f"ğŸ“Š è©•åˆ†å ±å‘Š:")
    print(f"   â¤ Faithfulness (ç„¡å¹»è¦º): {f_score:.2f} / 1.0")
    print(f"   â¤ Relevance (åˆ‡é¡Œåº¦):   {r_score:.2f} / 1.0")

    if f_score < 0.8:
        print("   âš ï¸  è­¦ç¤ºï¼šçƒå“¡å¯èƒ½ç”¢ç”Ÿå¹»è¦ºï¼")
    elif r_score < 0.5:
        print("   âš ï¸  è­¦ç¤ºï¼šçƒå“¡ç­”éæ‰€å•ï¼")
    else:
        print("   âœ… Passï¼šè¡¨ç¾å„ªè‰¯ã€‚")
    print("=" * 60)


# ==========================================
# ğŸ“ 10 é¡Œå®Œæ•´æ¸¬è©¦é¡Œåº« (ç°¡å–®ã€é™·é˜±ã€å¦å®š)
# ==========================================
questions = [
    # --- ç°¡å–®é¡Œ ---
    "è«‹å•å¯Œé‚¦å°Šå¾¡ä¸–ç•Œå¡çš„æ­£å¡å¹´è²»æ˜¯å¤šå°‘éŒ¢ï¼Ÿé™„å¡è¦å¹´è²»å—ï¼Ÿ",
    "æˆ‘è¦é ç´„é€£å‡æœŸé–“çš„ã€åœ‹å…§æ©Ÿå ´æ¥é€ã€ï¼Œæœ€æ™šéœ€è¦åœ¨å¹¾å€‹å·¥ä½œå¤©å‰é ç´„ï¼Ÿ",
    "è«‹å•é“è·¯æ•‘æ´æœå‹™å°ˆç·šçš„é›»è©±è™Ÿç¢¼æ˜¯å¤šå°‘ï¼Ÿ",

    # --- é™·é˜±é¡Œ (è€ƒé©— 12B çš„é‚è¼¯) ---
    "æˆ‘ä¸Šé€±å‰›è²·äº†æ©Ÿç¥¨ï¼Œé‡‘é¡æ˜¯ 12,000 å…ƒï¼Œæˆ‘æ˜¯å¯Œé‚¦ä¸–ç•Œå¡çš„å¡å‹ï¼ˆéç†è²¡æœƒå“¡ï¼‰ï¼Œè«‹å•æˆ‘å¯ä»¥é ç´„å…è²»æ©Ÿå ´æ¥é€å—ï¼Ÿ",
    "æˆ‘ç”¨å°Šå¾¡ä¸–ç•Œå¡åˆ·äº†æ©Ÿç¥¨ï¼Œä½†æ˜¯æ˜¯åœ¨ 7 å€‹æœˆå‰ï¼ˆç´„ 210 å¤©å‰ï¼‰åˆ·çš„ï¼Œç¾åœ¨è¦å‡ºåœ‹å¯ä»¥ç”¨æ©Ÿå ´å¤–åœåœè»Šå—ï¼Ÿ",
    "æˆ‘æ˜¯å¯Œé‚¦ç„¡é™å¡æŒå¡äººï¼Œæˆ‘å…’å­ä»Šå¹´ 26 æ­²æœªå©šï¼Œè·Ÿæˆ‘ä¸€èµ·å‡ºåœ‹ï¼Œæˆ‘å¹«ä»–åˆ·äº†å…¨é¡æ©Ÿç¥¨ï¼Œè«‹å•ä»–æœ‰æ—…éŠå¹³å®‰éšªçš„ä¿éšœå—ï¼Ÿ",
    "æˆ‘æŒæœ‰å¯Œé‚¦ä¸–ç•Œå¡ï¼Œä¸ŠæœŸå¸³å–®ä¸€èˆ¬æ¶ˆè²» 18,000 å…ƒï¼Œè«‹å•æˆ‘å»ã€å°ç£è¯é€šã€åœè»Šå¯ä»¥å…è²»åœå¹¾å°æ™‚ï¼Ÿ",

    # --- å¦å®šé¡Œ (æ’é™¤æ¢æ¬¾) ---
    "æˆ‘ç‚ºäº†æ¹Šå…å¹´è²»çš„é–€æª»ï¼Œå»å…¨è¯ç¦åˆ©ä¸­å¿ƒè²·äº†å¾ˆå¤šæ±è¥¿ï¼Œè«‹å•é€™äº›æ¶ˆè²»ç®—åœ¨ã€ä¸€èˆ¬æ¶ˆè²»ã€è£¡é¢å—ï¼Ÿ",
    "æˆ‘å‰›è²·çš„æ–°æ‰‹æ©Ÿè¢«å·äº†ï¼Œå¯ä»¥ç”¨ä¿¡ç”¨å¡çš„ã€å…¨çƒè³¼ç‰©ä¿éšœã€ç”³è«‹ç†è³ å—ï¼Ÿ",
    "æˆ‘çš„è»Šæœ‰æ”¹è£éï¼Œåº•ç›¤æ¯”è¼ƒä½ï¼ˆé›¢åœ° 15 å…¬åˆ†ï¼‰ï¼Œè»Šå­æ‹‹éŒ¨äº†å¯ä»¥ä½¿ç”¨å…è²»é“è·¯æ•‘æ´æ‹–åŠå—ï¼Ÿ"
]

# åŸ·è¡Œæ¸¬è©¦
for q in questions:
    run_rag_with_evaluation(q)
    time.sleep(3)

# æ¸…ç†
vectorstore.delete_collection()
print("\nâœ… æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")

