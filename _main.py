import os
import sys
import time
import re
import pdfplumber  # éœ€å®‰è£ pdfplumber
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from sentence_transformers import CrossEncoder
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# ğŸ” æª¢æŸ¥ API Key
GOOGLE_API_KEY = os.getenv("GEMINI_API_KEY")
if not GOOGLE_API_KEY:
    print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° GOOGLE_API_KEYï¼")
    sys.exit(1)

# ==========================================
# ğŸ› ï¸ è¨­å®šå€
# ==========================================
file_path = "fubon.pdf"
GENERATOR_MODEL = "gemma-3-12b-it"  # çƒå“¡
JUDGE_MODEL = "gemma-3-27b-it"  # è£åˆ¤

print(f"ğŸš€ å•Ÿå‹• RAG ç³»çµ± (Level 5.5 - Hybrid Search + Markdown Tables)...")
print(f"ğŸ“„ è™•ç†æª”æ¡ˆ: {file_path}")


# ==========================================
# 1. è³‡æ–™è™•ç† (é—œéµæ”¹è‰¯ï¼šè¡¨æ ¼è½‰ Markdown)
# ==========================================
def pdf_to_markdown_with_plumber(file_path):
    """
    ä½¿ç”¨ pdfplumber è®€å– PDFï¼Œå°‡åµæ¸¬åˆ°çš„è¡¨æ ¼è½‰æ›ç‚º Markdown æ ¼å¼ï¼Œ
    ä¸¦é™„åŠ åœ¨é é¢ç´”æ–‡å­—ä¹‹å¾Œã€‚
    """
    print(f"   [ç³»çµ±æç¤º] è§£æ PDF ä¸¦è½‰æ›è¡¨æ ¼ç‚º Markdown: {file_path} ...")
    docs = []

    try:
        with pdfplumber.open(file_path) as pdf:
            for i, page in enumerate(pdf.pages):
                # 1. æå–ç´”æ–‡å­— (ä¿ç•™åŸæœ¬å…§å®¹)
                text = page.extract_text() or ""

                # 2. æå–è¡¨æ ¼
                tables = page.extract_tables()
                table_markdowns = []

                for table in tables:
                    if not table: continue

                    # æ¸…ç†è¡¨æ ¼è³‡æ–™ (è™•ç† None)
                    clean_table = [[str(cell).strip() if cell else "" for cell in row] for row in table]

                    # è½‰ç‚º Markdown
                    if len(clean_table) > 0:
                        # è™•ç† Header
                        header = "| " + " | ".join(clean_table[0]) + " |"
                        separator = "| " + " | ".join(["---"] * len(clean_table[0])) + " |"

                        # è™•ç† Body
                        body_rows = []
                        for row in clean_table[1:]:
                            body_rows.append("| " + " | ".join(row) + " |")

                        body = "\n".join(body_rows)
                        md_table = f"\n{header}\n{separator}\n{body}\n"
                        table_markdowns.append(md_table)

                # 3. çµ„åˆå…§å®¹ï¼šç´”æ–‡å­— + æ¨™ç¤º + Markdown è¡¨æ ¼
                # é€™æ¨£åšçš„å¥½è™•æ˜¯ï¼šæ–‡å­—é¡ŒæŸ¥å¾—åˆ° textï¼Œè¡¨æ ¼é¡ŒæŸ¥å¾—åˆ° markdown
                full_content = text
                if table_markdowns:
                    full_content += "\n\n=== åµæ¸¬åˆ°çš„è¡¨æ ¼çµæ§‹ (Markdown) ===\n" + "\n".join(table_markdowns)

                docs.append(Document(
                    page_content=full_content,
                    metadata={"source": file_path, "page": i + 1}
                ))
    except Exception as e:
        print(f"âŒ PDF è§£æéŒ¯èª¤: {e}")
        sys.exit(1)

    return docs


def create_retriever(file_path):
    # 1. è¼‰å…¥ä¸¦è½‰æ›è³‡æ–™
    raw_docs = pdf_to_markdown_with_plumber(file_path)

    # 2. åˆ‡åˆ† (é—œéµï¼šåŠ å¤§ Chunk Size ä»¥å®¹ç´ Markdown è¡¨æ ¼)
    # åŸæœ¬ 400 å¤ªå°ï¼Œè¡¨æ ¼æœƒè¢«åˆ‡æ–·ã€‚æ”¹ç‚º 1000ã€‚
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
    )
    splits = text_splitter.split_documents(raw_docs)
    print(f"   [ç³»çµ±æç¤º] æ–‡ä»¶å·²åˆ‡åˆ†ç‚º {len(splits)} å€‹å€å¡Š (Chunk Size: 1000)")

    # 3. Embedding æ¨¡å‹
    print("   [ç³»çµ±æç¤º] è¼‰å…¥ Embedding æ¨¡å‹...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="intfloat/multilingual-e5-large",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    # 4. å»ºç«‹ Chroma (å‘é‡æª¢ç´¢)
    print("   [ç³»çµ±æç¤º] å»ºç«‹ Chroma å‘é‡ç´¢å¼•...")
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embedding_model,
        collection_name="fubon_markdown_hybrid"  # æ”¹å€‹åå­—ç¢ºä¿ä¸æ··ç”¨èˆŠè³‡æ–™
    )
    chroma_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

    # 5. å»ºç«‹ BM25 (é—œéµå­—æª¢ç´¢)
    print("   [ç³»çµ±æç¤º] å»ºç«‹ BM25 é—œéµå­—ç´¢å¼•...")
    bm25_retriever = BM25Retriever.from_documents(splits)
    bm25_retriever.k = 10

    # 6. æ··åˆæª¢ç´¢
    print("   [ç³»çµ±æç¤º] å•Ÿå‹• Hybrid Ensemble...")
    ensemble_retriever = EnsembleRetriever(
        retrievers=[chroma_retriever, bm25_retriever],
        weights=[0.5, 0.5]
    )

    return ensemble_retriever, vectorstore  # å›å‚³ vectorstore ä»¥ä¾¿æœ€å¾Œæ¸…ç†


# å»ºç«‹æª¢ç´¢å™¨
retriever, vector_db = create_retriever(file_path)

# è¼‰å…¥ Re-ranker
print("   [ç³»çµ±æç¤º] è¼‰å…¥ Cross-Encoder Re-ranker...")
reranker_model = CrossEncoder('BAAI/bge-reranker-base', device='cpu')

# ==========================================
# 2. æ¨¡å‹èˆ‡ Prompt è¨­å®š
# ==========================================
try:
    llm_generator = ChatGoogleGenerativeAI(
        model=GENERATOR_MODEL, temperature=0.1, google_api_key=GOOGLE_API_KEY
    )
    llm_judge = ChatGoogleGenerativeAI(
        model=JUDGE_MODEL, temperature=0.0, google_api_key=GOOGLE_API_KEY
    )
except Exception as e:
    print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
    sys.exit(1)

# Prompt Template (é‡å°è¡¨æ ¼å„ªåŒ–)
rag_template = """# Role
ä½ æ˜¯ä¸€ä½å°ˆæ¥­ã€é‚è¼¯åš´è¬¹çš„ã€Œå°åŒ—å¯Œé‚¦éŠ€è¡Œé ‚ç´šå¡æ¬Šç›Šå¯©æ ¸å°ˆå“¡ã€ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“š <context> æº–ç¢ºå›ç­”å®¢æˆ¶å•é¡Œã€‚

# Task
è«‹é–±è®€ <context>ï¼Œä¸¦é‡å° <question> é€²è¡Œè³‡æ ¼å¯©æ ¸èˆ‡å›è¦†ã€‚
<context> å¯èƒ½åŒ…å« Markdown æ ¼å¼çš„è¡¨æ ¼ï¼Œè«‹ä»”ç´°å°ç…§è¡¨æ ¼çš„æ¬„ä½èˆ‡æ•¸å€¼ã€‚

# Constraints
1. **åš´æ ¼å¼•ç”¨**ï¼šå›ç­”å¿…é ˆåŸºæ–¼ <context> å…§å®¹ï¼Œå›ç­”çµå°¾è«‹æ¨™è¨»ä¾†æºã€‚
2. **è¡¨æ ¼å°ç…§**ï¼šè‹¥è³‡æ–™ç‚º Markdown è¡¨æ ¼ï¼Œè«‹ç¢ºä¿ã€Œæ¬„ä½ã€èˆ‡ã€Œåˆ—ã€çš„å°æ‡‰é—œä¿‚æ­£ç¢º (ä¾‹å¦‚ï¼šç¢ºèªã€Œå¡åˆ¥ã€å°æ‡‰çš„ã€Œé–€æª»ã€)ã€‚
3. **æ•¸å€¼æ¯”å°**ï¼šè‹¥å•é¡Œæ¶‰åŠé‡‘é¡ã€å¤©æ•¸ï¼Œè«‹åœ¨æ€è€ƒéç¨‹ä¸­åˆ—å‡ºç®—å¼æ¯”å°ã€‚
4. **æ’é™¤æ¢æ¬¾**ï¼šç‰¹åˆ¥æª¢æŸ¥ã€Œä¸€èˆ¬æ¶ˆè²»å®šç¾©ã€çš„æ’é™¤é …ç›®ã€‚
5. **èª å¯¦å›ç­”**ï¼šè‹¥ <context> æœªæåŠï¼Œå›ç­”ã€Œæ‰‹å†Šä¸­æœªæåŠã€ã€‚

# Instruction (CoT)
åœ¨å›ç­”å‰ï¼Œè«‹å‹™å¿…å…ˆé€²è¡Œ <thinking> æ­¥é©Ÿï¼š
1. **è­˜åˆ¥è®Šæ•¸**ï¼šç”¨æˆ¶çš„å¡åˆ¥ã€èº«åˆ†ã€æ¶ˆè²»é‡‘é¡ã€æ™‚é–“é»ã€‚
2. **æŸ¥æ‰¾æ¢æ¬¾**ï¼šåœ¨ <context> ä¸­æ‰¾åˆ°å°æ‡‰è¦å‰‡ï¼ˆå„ªå…ˆæŸ¥çœ‹ Markdown è¡¨æ ¼ï¼‰ã€‚
3. **é‚è¼¯åˆ¤å®š**ï¼š
   - è³‡æ ¼æª¢æŸ¥ï¼šç”¨æˆ¶é‡‘é¡ vs é–€æª»ï¼Ÿ
   - æœŸé™æª¢æŸ¥ï¼šå¤©æ•¸ vs é™åˆ¶ï¼Ÿ
   - æ’é™¤æª¢æŸ¥ï¼šæ˜¯å¦åœ¨æ’é™¤åå–®ï¼Ÿ
4. **ç”Ÿæˆå›ç­”**ï¼šæ ¹æ“šåˆ¤å®šçµæœå›è¦†ã€‚

# Context
{context}

# Question
{question}

Answer:"""

rag_prompt = PromptTemplate.from_template(rag_template)


# ==========================================
# 3. è©•åˆ†é‚è¼¯ (RAGAS - Lite)
# ==========================================
def calculate_ragas_score(question, answer, contexts):
    # Faithfulness
    f_prompt = PromptTemplate.from_template("""
    ä½ æ˜¯ä¸€ä½åš´æ ¼çš„ RAG è©•æ¸¬å“¡ã€‚è«‹æª¢æŸ¥ã€ŒAI å›ç­”ã€æ˜¯å¦åŒ…å«ã€Œåƒè€ƒç‰‡æ®µã€ä¸­æ²’æœ‰çš„å¹»è¦ºè³‡è¨Šã€‚
    è‹¥åƒè€ƒç‰‡æ®µä¸­æœ‰ Markdown è¡¨æ ¼ï¼Œè«‹ç¢ºèª AI æ˜¯å¦æ­£ç¢ºè®€å–è¡¨æ ¼æ•¸æ“šã€‚
    ã€åƒè€ƒç‰‡æ®µã€‘ï¼š{contexts}
    ã€AI å›ç­”ã€‘ï¼š{answer}
    è«‹å›å‚³ 0.0 åˆ° 1.0 çš„åˆ†æ•¸ã€‚åªå›å‚³æ•¸å­—ã€‚
    """)

    # Relevance
    r_prompt = PromptTemplate.from_template("""
    ä½ æ˜¯ä¸€ä½åš´æ ¼çš„ RAG è©•æ¸¬å“¡ã€‚è«‹è©•åˆ†ã€ŒAI å›ç­”ã€æ˜¯å¦ç²¾æº–å›ç­”äº†ã€Œç”¨æˆ¶å•é¡Œã€ã€‚
    ã€ç”¨æˆ¶å•é¡Œã€‘ï¼š{question}
    ã€AI å›ç­”ã€‘ï¼š{answer}
    è«‹å›å‚³ 0.0 åˆ° 1.0 çš„åˆ†æ•¸ã€‚åªå›å‚³æ•¸å­—ã€‚
    """)

    try:
        f_chain = f_prompt | llm_judge | StrOutputParser()
        r_chain = r_prompt | llm_judge | StrOutputParser()

        f_str = f_chain.invoke({"contexts": contexts, "answer": answer}).strip()
        r_str = r_chain.invoke({"question": question, "answer": answer}).strip()

        f_match = re.findall(r"[-+]?\d*\.\d+|\d+", f_str)
        r_match = re.findall(r"[-+]?\d*\.\d+|\d+", r_str)

        return (float(f_match[0]) if f_match else 0.0, float(r_match[0]) if r_match else 0.0)
    except:
        return 0.0, 0.0


# ==========================================
# 4. ä¸»åŸ·è¡Œæµç¨‹
# ==========================================
def run_rag_with_evaluation(query):
    print(f"\nâ“ æ¸¬è©¦å•é¡Œ: {query}")
    print("-" * 50)

    # 1. Recall & Rerank
    initial_docs = retriever.invoke(query)
    pairs = [[query, doc.page_content] for doc in initial_docs]
    scores = reranker_model.predict(pairs)
    scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)
    top_3_docs = [doc for doc, score in scored_docs[:3]]

    context_text = "\n\n".join([doc.page_content for doc in top_3_docs])

    # Debug: å¯ä»¥åœ¨é€™è£¡å°å‡º context_text æª¢æŸ¥æ˜¯å¦æœ‰ Markdown è¡¨æ ¼
    # print(f"[DEBUG] Context Preview: {context_text[:200]}...")

    # 2. Generation
    print(f"ğŸ¤– çƒå“¡ (12B) ç”Ÿæˆå›ç­”ä¸­ (Streaming)...")
    print("-" * 20)
    chain = rag_prompt | llm_generator | StrOutputParser()
    full_answer = ""
    for chunk in chain.stream({"context": context_text, "question": query}):
        print(chunk, end="", flush=True)
        full_answer += chunk
    print()

    # 3. Evaluation
    print("-" * 50)
    print(f"âš–ï¸ è£åˆ¤ (27B) è©•åˆ†ä¸­...")
    f_score, r_score = calculate_ragas_score(query, full_answer, context_text)

    print(f"ğŸ“Š è©•åˆ†å ±å‘Š: F={f_score:.2f}, R={r_score:.2f}")
    if f_score < 0.8:
        print("   âš ï¸  è­¦ç¤ºï¼šå¯èƒ½ç”¢ç”Ÿå¹»è¦ºï¼")
    elif r_score < 0.5:
        print("   âš ï¸  è­¦ç¤ºï¼šç­”éæ‰€å•ï¼")
    else:
        print("   âœ… Passï¼šè¡¨ç¾å„ªè‰¯ã€‚")
    print("=" * 60)


# ==========================================
# 5. æ¸¬è©¦é¡Œåº«
# ==========================================
questions = [
    # ç°¡å–®é¡Œ
    "å°Šå¾¡ä¸–ç•Œå¡å¹´è²»å¤šå°‘ï¼Ÿ",
    # è¡¨æ ¼é¡Œ (é—œéµæ¸¬è©¦ Q2)
    "æˆ‘è¦é ç´„é€£å‡æœŸé–“çš„ã€åœ‹å…§æ©Ÿå ´æ¥é€ã€ï¼Œæœ€æ™šéœ€è¦åœ¨å¹¾å€‹å·¥ä½œå¤©å‰é ç´„ï¼Ÿ",
    "è«‹å•é“è·¯æ•‘æ´æœå‹™å°ˆç·šçš„é›»è©±è™Ÿç¢¼æ˜¯å¤šå°‘ï¼Ÿ",

    # é™·é˜±é¡Œ (è¡¨æ ¼ + é‚è¼¯)
    "æˆ‘ä¸Šé€±å‰›è²·äº†æ©Ÿç¥¨ï¼Œé‡‘é¡æ˜¯ 12,000 å…ƒï¼Œæˆ‘æ˜¯å¯Œé‚¦ä¸–ç•Œå¡çš„å¡å‹ï¼ˆéç†è²¡æœƒå“¡ï¼‰ï¼Œè«‹å•æˆ‘å¯ä»¥é ç´„å…è²»æ©Ÿå ´æ¥é€å—ï¼Ÿ",
    "æˆ‘ç”¨å°Šå¾¡ä¸–ç•Œå¡åˆ·äº†æ©Ÿç¥¨ï¼Œä½†æ˜¯æ˜¯åœ¨ 7 å€‹æœˆå‰ï¼ˆç´„ 210 å¤©å‰ï¼‰åˆ·çš„ï¼Œç¾åœ¨è¦å‡ºåœ‹å¯ä»¥ç”¨æ©Ÿå ´å¤–åœåœè»Šå—ï¼Ÿ",
    "æˆ‘æ˜¯å¯Œé‚¦ç„¡é™å¡æŒå¡äººï¼Œæˆ‘å…’å­ä»Šå¹´ 26 æ­²æœªå©šï¼Œè·Ÿæˆ‘ä¸€èµ·å‡ºåœ‹ï¼Œæˆ‘å¹«ä»–åˆ·äº†å…¨é¡æ©Ÿç¥¨ï¼Œè«‹å•ä»–æœ‰æ—…éŠå¹³å®‰éšªçš„ä¿éšœå—ï¼Ÿ",

    # è¡¨æ ¼é¡Œ (é—œéµæ¸¬è©¦ Q7 - é€™æ˜¯æœ€é›£çš„)
    "æˆ‘æŒæœ‰å¯Œé‚¦ä¸–ç•Œå¡ï¼Œä¸ŠæœŸå¸³å–®ä¸€èˆ¬æ¶ˆè²» 18,000 å…ƒï¼Œè«‹å•æˆ‘å»ã€å°ç£è¯é€šã€åœè»Šå¯ä»¥å…è²»åœå¹¾å°æ™‚ï¼Ÿ",

    # å¦å®šé¡Œ
    "æˆ‘ç‚ºäº†æ¹Šå…å¹´è²»çš„é–€æª»ï¼Œå»å…¨è¯ç¦åˆ©ä¸­å¿ƒè²·äº†å¾ˆå¤šæ±è¥¿ï¼Œè«‹å•é€™äº›æ¶ˆè²»ç®—åœ¨ã€ä¸€èˆ¬æ¶ˆè²»ã€è£¡é¢å—ï¼Ÿ",
    "æˆ‘å‰›è²·çš„æ–°æ‰‹æ©Ÿè¢«å·äº†ï¼Œå¯ä»¥ç”¨ä¿¡ç”¨å¡çš„ã€å…¨çƒè³¼ç‰©ä¿éšœã€ç”³è«‹ç†è³ å—ï¼Ÿ",
    "æˆ‘çš„è»Šæœ‰æ”¹è£éï¼Œåº•ç›¤æ¯”è¼ƒä½ï¼ˆé›¢åœ° 15 å…¬åˆ†ï¼‰ï¼Œè»Šå­æ‹‹éŒ¨äº†å¯ä»¥ä½¿ç”¨å…è²»é“è·¯æ•‘æ´æ‹–åŠå—ï¼Ÿ"
]

for q in questions:
    run_rag_with_evaluation(q)
    time.sleep(3)

# æ¸…ç†
vector_db.delete_collection()
print("\nâœ… æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")