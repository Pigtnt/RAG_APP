import re
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from sentence_transformers import CrossEncoder
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# åŒ¯å…¥è¨­å®šèˆ‡ Prompts
from config import Config
from prompts import rag_prompt, faithfulness_prompt, relevance_prompt

class RAGModel:
    def __init__(self):
        print("ğŸ”§ [Model] åˆå§‹åŒ– RAG å¼•æ“...")
        
        # --- 1. Embedding & Database ---
        # [Core] å‘é‡æ¨¡å‹ (ä½¿ç”¨ CPU)
        self.embedding_model = HuggingFaceEmbeddings(
            model_name=Config.EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # [Core] å‘é‡è³‡æ–™åº«é€£ç·š
        self.vector_store = Chroma(
            persist_directory=Config.DB_PATH,
            embedding_function=self.embedding_model,
            collection_name=Config.COLLECTION_NAME
        )
        
        # --- 2. Retrieval Strategy (Hybrid) ---
        print("ğŸ“š [Model] é‡å»ºç´¢å¼•èˆ‡ Re-ranker...")
        all_docs = self.vector_store.get()["documents"] 
        doc_objects = [Document(page_content=t) for t in all_docs]
        
        # [Param] åˆæ­¥æª¢ç´¢æ•¸é‡ (k=10)
        chroma_retriever = self.vector_store.as_retriever(search_kwargs={"k": 10})
        bm25_retriever = BM25Retriever.from_documents(doc_objects)
        bm25_retriever.k = 10
        
        # [Param] æ··åˆæª¢ç´¢æ¬Šé‡ (Vector=0.5, Keyword=0.5)
        self.ensemble_retriever = EnsembleRetriever(
            retrievers=[chroma_retriever, bm25_retriever],
            weights=[0.5, 0.5]
        )
        
        # --- 3. Refinement & Generation ---
        # [Core] Re-ranker æ¨¡å‹
        self.reranker = CrossEncoder(Config.RERANKER_MODEL, device='cpu')
        
        # [Param] å›ç­”ç”Ÿæˆæ¨¡å‹ (Temperature=0.1 é™ä½éš¨æ©Ÿæ€§)
        self.llm = ChatGoogleGenerativeAI(
            model=Config.GENERATOR_MODEL, 
            temperature=0.1, 
            google_api_key=Config.API_KEY
        )

        # [Param] è©•åˆ†è£åˆ¤æ¨¡å‹ (Temperature=0.0 è¦æ±‚çµ•å°ç†æ€§)
        print(f"âš–ï¸ [Model] åˆå§‹åŒ–è£åˆ¤æ¨¡å‹ ({Config.JUDGE_MODEL})...")
        self.judge_llm = ChatGoogleGenerativeAI(
            model=Config.JUDGE_MODEL, 
            temperature=0.0, 
            google_api_key=Config.API_KEY
        )

    def get_answer(self, query):
        """
        [Logic] RAG ä¸»æµç¨‹ï¼šæª¢ç´¢ -> é‡æ’åº -> ä¸Šä¸‹æ–‡çµ„è£ -> ç”Ÿæˆ (ä¸²æµ)
        """
        
        # Step 1: åˆæ­¥æ··åˆæª¢ç´¢ (Recall)
        initial_docs = self.ensemble_retriever.invoke(query)
        
        # Step 2: ç²¾ç¢ºé‡æ’åº (Rerank)
        # [Logic] è¨ˆç®— (Query, Doc) ç›¸ä¼¼åº¦åˆ†æ•¸ä¸¦æ’åº
        pairs = [[query, doc.page_content] for doc in initial_docs]
        scores = self.reranker.predict(pairs)
        scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)
        
        # [Param] æœ€çµ‚é¸å– Top-3 æ–‡ä»¶é€²å…¥ Prompt
        top_docs = [doc for doc, score in scored_docs[:3]]
        
        # Step 3: çµ„è£ Context
        context = "\n\n".join([doc.page_content for doc in top_docs])
        
        # Step 4: ç”Ÿæˆå›ç­” (Streaming)
        chain = rag_prompt | self.llm | StrOutputParser()
        original_stream = chain.stream({"context": context, "question": query})

        # [Logic] ä¸²æµéæ¿¾å™¨ï¼šç§»é™¤ <thinking> æ¨™ç±¤ï¼Œåªå›å‚³æœ€çµ‚ç­”æ¡ˆ
        def clean_stream(stream):
            buffer = ""
            thinking_ended = False
            
            print("\nğŸ¤– [LLM Raw Output Start] ------------------") # Debug
            
            for chunk in stream:
                print(chunk, end="", flush=True) # Debug: å³æ™‚å°å‡º

                if thinking_ended:
                    yield chunk
                    continue
                
                buffer += chunk
                
                # åµæ¸¬æ€è€ƒçµæŸæ¨™ç±¤
                if "</thinking>" in buffer:
                    thinking_ended = True
                    parts = buffer.split("</thinking>")
                    real_answer = parts[-1] 
                    if real_answer: yield real_answer
                    buffer = "" 

            print("\nğŸ¤– [LLM Raw Output End] --------------------") # Debug

            # [Safety Net] è‹¥æ¨¡å‹æœªè¼¸å‡ºçµæŸæ¨™ç±¤ï¼Œå‰‡å¼·è¡Œè¼¸å‡ºæ‰€æœ‰å…§å®¹é˜²æ­¢ç©ºç™½
            if not thinking_ended and buffer:
                print("\nâš ï¸ è­¦å‘Š: æœªåµæ¸¬åˆ° </thinking> æ¨™ç±¤ï¼Œç›´æ¥è¼¸å‡ºæ‰€æœ‰å…§å®¹ã€‚")
                yield buffer

        return clean_stream(original_stream), top_docs

    def calculate_score(self, question, answer, source_docs):
        """
        [Logic] RAGAS Lite è©•åˆ†æ©Ÿåˆ¶
        Returns: (Faithfulness Score, Relevance Score)
        """
        print(f"âš–ï¸ [Model] å•Ÿå‹•è£åˆ¤æ¨¡å‹ ({Config.JUDGE_MODEL}) é€²è¡Œè©•åˆ†...")
        contexts = "\n\n".join([doc.page_content for doc in source_docs])
        
        try:
            # å»ºæ§‹è©•åˆ† Chain
            f_chain = faithfulness_prompt | self.judge_llm | StrOutputParser()
            r_chain = relevance_prompt | self.judge_llm | StrOutputParser()

            # [Logic] åŒæ­¥åŸ·è¡Œè©•åˆ† (æœƒé˜»å¡ç›´åˆ° LLM å›å‚³)
            f_str = f_chain.invoke({"contexts": contexts, "answer": answer}).strip()
            r_str = r_chain.invoke({"question": question, "answer": answer}).strip()

            # [Logic] ä½¿ç”¨ Regex æå–æ•¸å€¼ (é˜²æ­¢ LLM è¼¸å‡ºå¤šé¤˜æ–‡å­—)
            f_match = re.findall(r"[-+]?\d*\.\d+|\d+", f_str)
            r_match = re.findall(r"[-+]?\d*\.\d+|\d+", r_str)

            f_score = float(f_match[0]) if f_match else 0.0
            r_score = float(r_match[0]) if r_match else 0.0
            
            return f_score, r_score
            
        except Exception as e:
            print(f"âš ï¸ è©•åˆ†éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            return 0.0, 0.0