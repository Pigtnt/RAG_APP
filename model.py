from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from sentence_transformers import CrossEncoder
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# åŒ¯å…¥æ‚¨çš„è¨­å®šèˆ‡ Prompt
from config import Config
from prompts import rag_prompt  # âœ… å¾ prompts.py åŒ¯å…¥ Prompt Template

class RAGModel:
    def __init__(self):
        print("ğŸ”§ [Model] åˆå§‹åŒ– RAG å¼•æ“...")
        
        # 1. æº–å‚™ Embedding
        self.embedding_model = HuggingFaceEmbeddings(
            model_name=Config.EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # 2. è¼‰å…¥ Chroma DB
        self.vector_store = Chroma(
            persist_directory=Config.DB_PATH,
            embedding_function=self.embedding_model,
            collection_name=Config.COLLECTION_NAME
        )
        
        # 3. æº–å‚™ Retriever (Hybrid)
        print("ğŸ“š [Model] é‡å»ºç´¢å¼•èˆ‡ Re-ranker...")
        all_docs = self.vector_store.get()["documents"] 
        doc_objects = [Document(page_content=t) for t in all_docs]
        
        chroma_retriever = self.vector_store.as_retriever(search_kwargs={"k": 10})
        bm25_retriever = BM25Retriever.from_documents(doc_objects)
        bm25_retriever.k = 10
        
        self.ensemble_retriever = EnsembleRetriever(
            retrievers=[chroma_retriever, bm25_retriever],
            weights=[0.5, 0.5]
        )
        
        # 4. Re-ranker
        self.reranker = CrossEncoder(Config.RERANKER_MODEL, device='cpu')
        
        # 5. LLM
        self.llm = ChatGoogleGenerativeAI(
            model=Config.GENERATOR_MODEL, 
            temperature=0.1, 
            google_api_key=Config.API_KEY
        )

    def get_answer(self, query):
        """æ¥æ”¶ queryï¼Œå›å‚³ (å›ç­”ä¸²æµ, åƒè€ƒæ–‡ä»¶)"""
        
        # Step 1: æª¢ç´¢
        initial_docs = self.ensemble_retriever.invoke(query)
        
        # Step 2: Rerank
        pairs = [[query, doc.page_content] for doc in initial_docs]
        scores = self.reranker.predict(pairs)
        scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)
        top_docs = [doc for doc, score in scored_docs[:3]]
        
        # çµ„è£ Context
        context = "\n\n".join([doc.page_content for doc in top_docs])
        
        # Step 3: ç”Ÿæˆ
        chain = rag_prompt | self.llm | StrOutputParser()
        original_stream = chain.stream({"context": context, "question": query})

        # âœ… åŠ å…¥éæ¿¾é‚è¼¯ï¼šåªå›å‚³ </thinking> ä¹‹å¾Œçš„å…§å®¹
        def clean_stream(stream):
            buffer = ""
            thinking_ended = False
            
            print("\nğŸ¤– [LLM Raw Output Start] ------------------") # Debug ç”¨
            
            for chunk in stream:
                # 1. Debug: åœ¨çµ‚ç«¯æ©Ÿå³æ™‚å°å‡ºï¼Œç¢ºèªå¾Œç«¯æœ‰æ”¶åˆ°å­—
                print(chunk, end="", flush=True) 

                if thinking_ended:
                    yield chunk
                    continue
                
                buffer += chunk
                
                # 2. åµæ¸¬æ€è€ƒçµæŸæ¨™ç±¤
                if "</thinking>" in buffer:
                    thinking_ended = True
                    # å–å‡ºæ¨™ç±¤å¾Œçš„çœŸæ­£å›ç­”
                    parts = buffer.split("</thinking>")
                    real_answer = parts[-1] 
                    
                    # æœ‰æ™‚å€™æ¨™ç±¤å¾Œç·Šæ¥è‘—æ›è¡Œï¼Œå¯ä»¥ä¿ç•™æˆ– strip() çœ‹æ‚¨éœ€æ±‚
                    if real_answer:
                        yield real_answer
                    
                    buffer = "" # æ¸…ç©ºç·©è¡å€ï¼Œé‡‹æ”¾è¨˜æ†¶é«”

            print("\nğŸ¤– [LLM Raw Output End] --------------------") # Debug ç”¨

            # 3. ğŸ”¥ é—œéµå®‰å…¨ç¶² (Safety Net) ğŸ”¥
            # å¦‚æœä¸²æµçµæŸäº†ï¼Œä½† thinking_ended é‚„æ˜¯ False (ä»£è¡¨æ¨¡å‹æ²’ä¹–ä¹–è¼¸å‡º </thinking>)
            # æ­¤æ™‚å¿…é ˆæŠŠ buffer è£¡çš„å…§å®¹å…¨éƒ¨åå‡ºä¾†ï¼Œä¸ç„¶ä½¿ç”¨è€…æœƒçœ‹åˆ°ç©ºç™½ï¼
            if not thinking_ended and buffer:
                print("\nâš ï¸ è­¦å‘Š: æœªåµæ¸¬åˆ° </thinking> æ¨™ç±¤ï¼Œç›´æ¥è¼¸å‡ºæ‰€æœ‰å…§å®¹ã€‚")
                yield buffer

        # å›å‚³ã€Œéæ¿¾å¾Œã€çš„ä¸²æµ
        return clean_stream(original_stream), top_docs