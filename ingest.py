import sys
import pdfplumber
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from config import Config

def load_pdf_with_tables(file_path):
    """
    [Logic] ç‰¹æ®Šè™•ç†ï¼šPDF è¡¨æ ¼è½‰ Markdown
    åŠŸèƒ½ï¼šé™¤äº†è®€å–ç´”æ–‡å­—ï¼Œé‚„å°‡è¡¨æ ¼è½‰æ›ç‚º Markdown æ ¼å¼ï¼Œ
    å„ªé»ï¼šå¤§å¹…æå‡ LLM å°çµæ§‹åŒ–æ•¸æ“š (å¦‚è²»ç‡è¡¨ã€è³‡æ ¼è¡¨) çš„ç†è§£èƒ½åŠ›ã€‚
    """
    print(f"ğŸ“„ è§£æ PDF: {file_path}")
    docs = []
    with pdfplumber.open(file_path) as pdf:
        for i, page in enumerate(pdf.pages):
            text = page.extract_text() or ""
            
            # æå–è¡¨æ ¼ä¸¦è½‰ç‚º Markdown
            tables = page.extract_tables()
            table_markdowns = []
            for table in tables:
                if not table: continue
                clean_table = [[str(cell).strip() if cell else "" for cell in row] for row in table]
                if len(clean_table) > 0:
                    header = "| " + " | ".join(clean_table[0]) + " |"
                    separator = "| " + " | ".join(["---"] * len(clean_table[0])) + " |"
                    body = "\n".join(["| " + " | ".join(row) + " |" for row in clean_table[1:]])
                    table_markdowns.append(f"\n{header}\n{separator}\n{body}\n")
            
            # çµ„åˆå…§å®¹ï¼šç´”æ–‡å­— + è¡¨æ ¼ Markdown
            full_content = text
            if table_markdowns:
                full_content += "\n\n=== è¡¨æ ¼çµæ§‹ (Markdown) ===\n" + "\n".join(table_markdowns)
            
            # å°è£ç‚º Document ç‰©ä»¶
            docs.append(Document(page_content=full_content, metadata={"source": file_path, "page": i + 1}))
    return docs

def main():
    print("ğŸš€ é–‹å§‹å»ºç«‹å‘é‡è³‡æ–™åº« (Ingestion)...")
    
    # 1. è®€å–èˆ‡è™•ç†
    raw_docs = load_pdf_with_tables(Config.PDF_PATH)
    
    # 2. åˆ‡åˆ† (Text Splitting)
    # [Param] chunk_size: 1000 å­—å…ƒã€‚è‹¥è¡¨æ ¼å¾ˆå¤§ï¼Œå»ºè­°è¨­å¤§ä¸€é»ä»¥å…è¡¨æ ¼è¢«åˆ‡æ–·ã€‚
    # [Param] chunk_overlap: 200 å­—å…ƒã€‚ä¿ç•™ä¸Šä¸‹æ–‡é‡ç–Šï¼Œé¿å…èªæ„æ–·è£‚ã€‚
    # [Param] separators: åˆ‡åˆ†å„ªå…ˆç´š (æ®µè½ -> å¥å­ -> ç©ºæ ¼)ã€‚
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=200, 
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""])
    splits = text_splitter.split_documents(raw_docs)
    print(f"ğŸ“¦ åˆ‡åˆ†å®Œæˆï¼šå…± {len(splits)} å€‹å€å¡Š")

    # 3. Embedding
    # [Core] å°‡æ–‡å­—è½‰ç‚ºå‘é‡æ•¸å€¼
    print("ğŸ§  è¼‰å…¥ Embedding æ¨¡å‹...")
    embedding_model = HuggingFaceEmbeddings(
        model_name=Config.EMBEDDING_MODEL,
        model_kwargs={'device': 'cpu'}, # [Param] è‹¥æœ‰ GPU å¯æ”¹ç‚º 'cuda'
        encode_kwargs={'normalize_embeddings': True}
    )

    # 4. å­˜å…¥ Chroma (Vector Database)
    # [Core] å¯«å…¥ç¡¬ç¢Ÿ (Persist)ï¼Œä¾›å¾ŒçºŒæŸ¥è©¢ä½¿ç”¨
    print(f"ğŸ’¾ å¯«å…¥è³‡æ–™åº«è‡³ {Config.DB_PATH} ...")
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embedding_model,
        collection_name=Config.COLLECTION_NAME, # [Param] è³‡æ–™é›†åç¨±
        persist_directory=Config.DB_PATH        # [Param] å„²å­˜è·¯å¾‘
    )
    
    print("âœ… è³‡æ–™åº«å»ºç«‹å®Œæˆï¼è«‹åŸ·è¡Œ uv run streamlit run app.py")

if __name__ == "__main__":
    main()